{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. PyTorch Going Modular\n",
    "\n",
    "Notebook Reference: https://www.learnpytorch.io/05_pytorch_going_modular/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within ML, modularity involves turning notebook code made up of a series of cells into Python files. \n",
    "For example, for `04_PyTorch_Custom_Datasets.ipynb` we can transform the cells into the following files:\n",
    "\n",
    "* `get_data.py` - a file that downloads data if needed\n",
    "* `data_setup.py` - a file to prepare and download data if needed\n",
    "* `engine.py` - a file containing various training functions\n",
    "* `model_builder.py` or `model.py` - a file to create a PyTorch model\n",
    "* `train.py` - a file to leverage all other files and train a target PyTorch model\n",
    "* `utils.py` - a file dedicated to helpful utility functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modularity is valuable since it makes these scripts more reproducible and easier to run. However, companies such as Netflix do defend the notion of using notebooks as their production code. \n",
    "\n",
    "Below are some pros and cons of both Notebooks and Python scripts.\n",
    "\n",
    "Notebooks:\n",
    "* Easier to experiment and get started; easier to share through a Google Collab notebook; very visual\n",
    "* Versioning can be hard; it is hard to use only specific parts; text and graphics can get in the way\n",
    "\n",
    "Python scripts:\n",
    "* Can package together, avoiding constantly rewriting code; can use git for versioning; used by many open source projects; larger projects can be run on cloud vendors\n",
    "* Experimenting is not as visual as notebooks since you have to run the entire script rather than one cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common structure for running PyTorch models written in Python scripts is the following:\n",
    "\n",
    "`python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS`\n",
    "\n",
    "Within this command, the double-dash values are known as argument flags, and the capitalized argument following them are the various hyper parameters we can set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Cell mode vs Script mode\n",
    "\n",
    "A cell mode notebook is a notebook run normally, where each cell in the notebook is either code or markdown.\n",
    "\n",
    "A script mode notebook is very similar to a cell mode notebook, however, many of the code cells may be turned into Python scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/get_data.py \n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to data folder\n",
    "data_path = Path(\"data\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "# Dataset\n",
    "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "    print(\"Downloading pizza, steak, sushi data...\")\n",
    "    f.write(request.content)\n",
    "\n",
    "# Unzipping\n",
    "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "    print(\"Unzipping pizza, steak, sushi data...\") \n",
    "    zip_ref.extractall(image_path)\n",
    "\n",
    "# Remove zip file\n",
    "os.remove(data_path / \"pizza_steak_sushi.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Datasets and DataLoaders (`data_setup.py`)\n",
    "\n",
    "To make the transformation process for our training and testing `Dataset`'s and `DataLoader`'s, we can make one function called `create_dataloaders()`.\n",
    "\n",
    "We can write it to file using the line `%%writefile going_modular/data_setup.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/data_setup.py\n",
    "\"\"\"\n",
    "Creates PyTorch DataLoaders for image classification data.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "class_names = ''\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, # training directory path\n",
    "    test_dir: str, # testing directory path\n",
    "    transform: transforms.Compose, # transforms to perform on training and testing\n",
    "    batch_size: int, # number of samples per batch in each DataLoader\n",
    "    num_workers: int=NUM_WORKERS # number of workers per DataLoader\n",
    "):\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  Creates the training and testing DataLoaders.\n",
    "\n",
    "  It takes in a training directory and testing directory path and turns them \n",
    "  into PyTorch Datasets and then into PyTorch DataLoaders.\n",
    "\n",
    "  It returns a tuple of (train_dataloader, test_dataloader, class_names) where\n",
    "  class_names is a list of the target classes. \n",
    "  \n",
    "  \"\"\"\n",
    "\n",
    "  # Use ImageFolder to create dataset(s)\n",
    "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "  class_names = train_data.classes\n",
    "\n",
    "  train_dataloader = DataLoader(\n",
    "      train_data, \n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True\n",
    "  )\n",
    "\n",
    "  test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True\n",
    "  )\n",
    "\n",
    "  return train_dataloader, test_dataloader, class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from going_modular import data_setup\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=\"data/pizza_steak_sushi/train\",\n",
    "    test_dir=\"data/pizza_steak_sushi/test\",\n",
    "    transform=data_transform,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=os.cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Making a model (`model_builder.py`)\n",
    "\n",
    "Notebooks 3 and 4 were built upon the TinyVGG model. Hence, for reusability, it makes sense to place such into a file so we can reuse it again and again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/model_builder.py\n",
    "\"\"\"\n",
    "Contains PyTorch model code to instantiate a TinyVGG model. \n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Create the TinyVGG architecture.\n",
    "\n",
    "    Replicates the TinyVGG architecture from the following website:\n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: int, \n",
    "        hidden_units: int,\n",
    "        output_shape: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(\n",
    "                kernel_size=2,\n",
    "                stride=2\n",
    "            )   \n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units, \n",
    "                kernel_size=3,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units * 13 * 13,\n",
    "            out_features=output_shape)\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of coding the TinyVGG model from scratch each time, it can now be imported as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from going_modular import model_builder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = model_builder.TinyVGG(input_shape=3,\n",
    "                              hidden_units=10,\n",
    "                              output_shape=len(class_names)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating `train_step()` and `test_step()` functions and `train()` to combine them\n",
    "\n",
    "1. `train_step()` takes in a model, a `DataLoader`, a loss function and an optimizer and trains the model on the DataLoaser\n",
    "2. `test_step()` takes in a model, a `DataLoader` and a loss function and evaluates the model on the `DataLoader`\n",
    "3. `train()` performs the other two steps together for a given number of epochs and returns a results dictionary\n",
    "\n",
    "Since these will be the engine of the model training, they will all be placed into a Python script called `engine.py` with the line `%%writefile going_modular/engine.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/engine.py\n",
    "\n",
    "\"\"\" \n",
    "Contains functions for training and testing a PyTorch model.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_step(\n",
    "    model: torch.nn.Module, # model to be trained\n",
    "    dataloader: torch.utils.data.DataLoader, # DataLoader instance for the model to be trained on\n",
    "    loss_fn: torch.nn.Module, # loss function to minimize\n",
    "    optimizer: torch.optim.Optimizer, # optimizer to help minimize the loss function\n",
    "    device: torch.device\n",
    "    ) -> Tuple[float, float]:\n",
    "\n",
    "    \"\"\" \n",
    "\n",
    "    Trains a PyTorch model for a single epoch\n",
    "\n",
    "    Turns a target PyTorch model to training mode and then runs through all of the training steps:\n",
    "        Forward Pass\n",
    "        Loss Calculation\n",
    "        Optimizer Step\n",
    "\n",
    "    It returns a tuple of training loss and training accuracy metrics\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # Calculate and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(\n",
    "    model: torch.nn.Module, # model to be tested\n",
    "    dataloader: torch.utils.data.DataLoader, # DataLoader instance for the model to be tested on\n",
    "    loss_fn: torch.nn.Module, # loss function to calculate loss on the test data\n",
    "    device: torch.device \n",
    "    ) -> Tuple[float, float]:\n",
    "\n",
    "    \"\"\" \n",
    "\n",
    "    Test a PyTorch model for a single epoch\n",
    "\n",
    "    Turns a target PyTorch model to \"eval\" mode and then performs a forward pass on a testing dataset\n",
    "\n",
    "    It returns a tuple of testing loss and testing accuracy metrics\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss = loss.item()\n",
    "\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc /= len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module, # model to be trained and tested\n",
    "    train_dataloader: torch.utils.data.DataLoader, # DataLoader instance for the model to be trained on\n",
    "    test_dataloader: torch.utils.data.DataLoader, # DataLoader instance for the model to be tested on\n",
    "    optimizer: torch.optim.Optimizer, # optimizer to help minimize the loss function\n",
    "    loss_fn: torch.nn.Module, # loss function to calculate loss on both datasets\n",
    "    epochs: int,\n",
    "    device: torch.device\n",
    "    ) -> Dict[str, List]:\n",
    "\n",
    "    \"\"\" \n",
    "    \n",
    "    Trains and tests a PyTorch model\n",
    "\n",
    "    Passes a target PyTorch model through the train_step() and test_step() functions for a number of epochs,\n",
    "    training and testing the model in the same epoch loop\n",
    "\n",
    "    It calculates, prints and stores evaluation metrics throughout\n",
    "\n",
    "    It returns a dictionary of training and testing loss as well as training and testing accuracy metrics.\n",
    "    Each metric has a value in a list for each epoch\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [], \n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        \n",
    "        train_loss, train_acc = train_step(\n",
    "            model=model, \n",
    "            dataloader=train_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        test_loss, test_acc = test_step(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} | train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f} | test_loss: {test_loss:.4f} | test_acc: {test_acc:.4f}\")\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine\n",
    "\n",
    "# engine.train(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a function to save the model (`utils.py`)\n",
    "\n",
    "We can use the `save_model()` function to make a file called `utils.py` with the line `%%writefile going_modular/utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/utils.py\n",
    "\"\"\" \n",
    "Contains various utility functions for PyTorch model training and saving\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(\n",
    "    model: torch.nn.Module, # model to save\n",
    "    target_dir: str, # directory for saving the model to\n",
    "    model_name: str # filename for the saved model. Should include either \".pth\" or \".pt\" as the file extension\n",
    "    ):\n",
    "\n",
    "    \"\"\" \n",
    "    Saves a PyTorch model to a target directory\n",
    "    \"\"\"\n",
    "\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    assert model.name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    torch.save(obj=model.state_dict(), f=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to use our `save_model()` function, we would import it and use it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import utils\n",
    "\n",
    "# save_model(\n",
    "#    model=...,\n",
    "#    target_dir=...,\n",
    "#    model_name=... \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train, evaluate and save the model (`train.py`)\n",
    "\n",
    "There are a lot of PyTorch repositories that combine all of their functionality together in a `train.py` file. Essentially, this file trains the model using whatever data is available.\n",
    "\n",
    "In our `train.py` file, we will combine all other Python scripts. \n",
    "\n",
    "This way a PyTorch model can be trained using a single line:\n",
    "\n",
    "`python train.py`\n",
    "\n",
    "To create `train.py`, we will do the following steps:\n",
    "\n",
    "1. Import the various dependencies\n",
    "2. Setup various hyper parameters\n",
    "3. Setup training and test directories\n",
    "4. Setup device-agnostic code\n",
    "5. Create the necessary data transforms\n",
    "6. Create the DataLoaders using `data_setup.py`\n",
    "7. Create the model using `model_builder.py`\n",
    "8. Setup the loss function and optimizer\n",
    "9. Train the model using `engine.py`\n",
    "10. Save the model using `utils.py`\n",
    "\n",
    "We can create the file from a notebook cell using `%%writefile going_modular/train.py`\n",
    "\n",
    "With the introduction of the `argparse` module, one can invoke the hyperparameters from the command line in the following way:\n",
    "\n",
    "`python train.py --num_epochs 5 --batch_size 32 --hidden_units 32 --learning_rate 0.001`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/train.py\n",
    "\"\"\" \n",
    "Trains a PyTorch image classification model using device-agnostic code.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import data_setup, engine, model_builder, utils\n",
    "from torchvision import transforms\n",
    "\n",
    "# Parser\n",
    "parser = argparse.ArgumentParser(description=\"Enter hyperparameters.\")\n",
    "\n",
    "parser.add_argument(\"--num_epochs\", default=5, type=int, help=\"The number of epochs to train for\")\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int, help=\"The number of samples per batch\")\n",
    "parser.add_argument(\"--hidden_units\", default=32, type=int, help=\"The number of hidden units in hidden layers\")\n",
    "parser.add_argument(\"--learning_rate\", default=0.001, type=float, help=\"The learning rate to use for the model\")\n",
    "parser.add_argument(\"--train_dir\", default=\"../data/pizza_steak_sushi/train\", type=str, help=\"The directory file path to training data in standard image classification format\")\n",
    "parser.add_argument(\"--test_dir\", default=\"../data/pizza_steak_sushi/test\", type=str, help=\"The directory file path to testing data in standard image classification format\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Hyper parameters\n",
    "NUM_EPOCHS = args.num_epochs\n",
    "BATCH_SIZE = args.batch_size\n",
    "HIDDEN_UNITS = args.hidden_units\n",
    "LEARNING_RATE = args.learning_rate\n",
    "\n",
    "# Directories\n",
    "train_dir = args.train_dir\n",
    "test_dir = args.test_dir\n",
    "# print(f\"[INFO]\\nEpochs:{NUM_EPOCHS}\\nBatch size:{BATCH_SIZE}\\nHidden units:{HIDDEN_UNITS}\\nLearning rate:{LEARNING_RATE}\\nTrain directory:{train_dir}\\nTest directory:{test_dir}\\n\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Transforms\n",
    "data_transform = transforms.Compose([\n",
    "  transforms.Resize((64, 64)),\n",
    "  transforms.ToTensor()\n",
    "])\n",
    "\n",
    "if __name__ == '__main__': # When num_workers > 0, the multiprocessing module can encounter issues by spawning worker processes. Wrapping it in this guard provides a clear entry point to avoid re-importing the module and leading to errors. \n",
    "  # DataLoaders with help from data_setup.py\n",
    "  train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "      train_dir=train_dir,\n",
    "      test_dir=test_dir,\n",
    "      transform=data_transform,\n",
    "      batch_size=BATCH_SIZE\n",
    "  )\n",
    "\n",
    "  # Model with help from model_builder.py\n",
    "  model = model_builder.TinyVGG(\n",
    "      input_shape=3,\n",
    "      hidden_units=HIDDEN_UNITS,\n",
    "      output_shape=len(class_names)\n",
    "  ).to(device)\n",
    "\n",
    "  # Loss and optimizer\n",
    "  loss_fn = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),\n",
    "                              lr=LEARNING_RATE)\n",
    "\n",
    "  # Training with help from engine.py\n",
    "  engine.train(model=model,\n",
    "              train_dataloader=train_dataloader,\n",
    "              test_dataloader=test_dataloader,\n",
    "              loss_fn=loss_fn,\n",
    "              optimizer=optimizer,\n",
    "              epochs=NUM_EPOCHS,\n",
    "              device=device)\n",
    "\n",
    "  # Save the model with help from utils.py\n",
    "  utils.save_model(model=model,\n",
    "                  target_dir=\"models\",\n",
    "                  model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
