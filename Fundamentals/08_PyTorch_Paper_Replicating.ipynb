{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebRIotgQzIC8"
      },
      "source": [
        "# PyTorch Paper Replicating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBBZbb3bzIC9"
      },
      "source": [
        "This file will focus on replicating a Machine Learning research paper and creating a Vision Transformer (ViT) from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtNEmHeyzIC9"
      },
      "source": [
        "### What is paper replicating?\n",
        "\n",
        "The goal of paper replicating is to replicate the most recent advances in Machine Learning which tend to be published in papers. Instead of using someone else's code, the focus here is to transform images, diagrams, math and text into usable code, in this case, PyTorch code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbr_A8JFzIC9"
      },
      "source": [
        "### Where can code examples of Machine Learning research papers be found?\n",
        "\n",
        "There is a lot of research in Machine Learning, so staying on top of the most recent advances is not easy. Said this, there are several places to find relevant papers.\n",
        "\n",
        "Some useful resources include:\n",
        "* arXiv\n",
        "* AK Twitter\n",
        "* Papers with Code\n",
        "* lucidrains' `vit-pytorch` GitHub repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXbMOpc3zIC9"
      },
      "source": [
        "### Which paper will be replicated?\n",
        "\n",
        "The paper this file will focus on is An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT paper)\n",
        "https://arxiv.org/abs/2010.11929\n",
        "\n",
        "The Transformer neural network architecture was originally introduced in the Machine Learning research paper Attention is all you need\n",
        "https://arxiv.org/abs/1706.03762\n",
        "\n",
        "A Transformer architecture is generally considered to be any neural network that uses the attention mechanism as its primary learning layer. This is similar to how Convolutional Neural Networks use convolutions as their primary learning layer.\n",
        "\n",
        "The Attention Mechanism is more thoroughly explained here:\n",
        "https://www.geeksforgeeks.org/ml-attention-mechanism/\n",
        "\n",
        "As suggested by its name, the Vision Transformer (ViT) architecture was designed to adapt the original Transformer architecture to vision problems.\n",
        "\n",
        "The original Vision Transformer has been through several iterations over the past couple of years, but we will focus on replicating the original.\n",
        "\n",
        "For the rest of this project, we will refer to the Vision Transformer as ViT while referring to the original Machine Learning research paper as ViT paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qeeBxkZzIC-"
      },
      "source": [
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EK-0mnmVzIC-",
        "outputId": "52b91302-9bf0-4153-c6c4-c77928736bc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.5.1+cu124\n",
            "torchvision version: 0.20.1+cu124\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[0]) >= 2, \"torch version should be 2.+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 15, \"torchvision version should be 0.15+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not correct. Installing correct versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "2WwbYzGLymHj"
      },
      "outputs": [],
      "source": [
        "import matplotlib as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "try:\n",
        "    from going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    print(\"[INFO] Could not find going_modular scripts. Downloading them from GitHub.\")\n",
        "    !git clone https://github.com/Aaron-Serpilin/Zero-To-Mastery-Pytorch\n",
        "    !mv Zero-To-Mastery-Pytorch/Fundamentals/going_modular .\n",
        "    !mv Zero-To-Mastery-Pytorch/Fundamental/helper_functions.py .\n",
        "    !rm -rf Zero-To-Mastery-Pytorch\n",
        "    from going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "2-5xzAefzIC-",
        "outputId": "ed6e7f43-224b-46ce-b57a-66f070f78893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfBUcOeHzIC-"
      },
      "source": [
        "## 1. Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "sOP7YA-5zIC-",
        "outputId": "4a1a1288-a5e2-4eca-8c6e-70fbad1fe4b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Did not find data/pizza_steak_sushi directory, creating one...\n",
            "[INFO] Downloading pizza_steak_sushi.zip from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip...\n",
            "[INFO] Unzipping pizza_steak_sushi.zip data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('data/pizza_steak_sushi')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCe-LJ-szIC-"
      },
      "outputs": [],
      "source": [
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QyMuSkTzIC-"
      },
      "source": [
        "## 2. Create Datasets and DataLoaders"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}