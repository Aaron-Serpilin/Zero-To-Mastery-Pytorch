{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model Deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the FoodVision Mini Project has only been accessible to us. Hence, the goal of this chapter is to deploy our FoodVision Mini model to the internet as a usable app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Machine Learning model deployment?\n",
    "\n",
    "Machine learning model deployment is the process of making your machine learning model accessible to someone or something else, allowing said user to interact with the model in some way.\n",
    "\n",
    "This can come in the form of a person, or a program, app, or model that interacts with our model. Machine learning model deployment involves making your model available to someone or something else. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why deploy a Machine Learning model?\n",
    "\n",
    "While evaluating a model on a well crafted test set, or visualizing its results can give a good indicator as to a model's performance, one can never truly know the model's performance unless it is released in the wild. \n",
    "\n",
    "Having people who have never used your model interact with it often reveals edge cases never thought of during training. Model deployment helps figure out errors in models that are not obvious during training/testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of Machine Learning model deployment\n",
    "\n",
    "There are many types of model deployment, but when deciding the optimal type, one has to start with the question:\n",
    "\n",
    "> What is the most ideal scenario for my machine learning model to be used?\n",
    "\n",
    "And then work backwards from there. In the case of FoodVision Mini, the ideal scenario would entail:\n",
    "\n",
    "* Someone takes a photo on a mobile device\n",
    "* The prediction comes back fast\n",
    "\n",
    "Therefore, this yields two important criteria:\n",
    "1. The model should work on a mobile device (leading to compute constraints)\n",
    "2. The model should make predictions fast (because a slow app is not very useful)\n",
    "\n",
    "When dealing with this criteria, we have to also account for where is the data going to be stored, and if the predictions can be returned immediately or later. \n",
    "\n",
    "Because of all these criteria to tackle, it is often better to start with the most ideal use case, and work backwards from there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is it going to go?\n",
    "\n",
    "Where does the model live when it is deployed?\n",
    "\n",
    "The main debate here is whether is lives on-device (also called edge/in the browser) or on the cloud (a computer/sever that isn't the actual device someone/something calls the model from).\n",
    "\n",
    "Both scenarios have their pros and cons. \n",
    "\n",
    "| Deployment Location | Pros                                                       | Cons                                                                                                               |\n",
    "|---------------------|------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|\n",
    "| On-device           | Can be very fast (since no data leaves the device)         | Limited compute power (larger models take longer to run)                                                           |\n",
    "|                     | Privacy preserving (again no data has to leave the device) | Limited storage space (smaller model size required)                                                                |\n",
    "|                     | No internet connection required (sometimes)                | Device-specific skills often required                                                                              |\n",
    "| On cloud            | Near unlimited compute power (can scale up when needed)    | Costs can get out of hand (if proper scaling limits aren't enforced)                                               |\n",
    "|                     | Can deploy one model and use everywhere (via API)          | Predictions can be slower due to data having to leave device and predictions having to come back (network latency) |\n",
    "|                     | Links into existing cloud ecosystem                        | Data has to leave device (this may cause privacy concerns)                                                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these considerations, there is an evident trade-off between performance and prediction time with on-device being less performant but faster while on cloud offers a more performant model that requires more computation and storage, leading to longer prediction times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is it going to function?\n",
    "\n",
    "When deploying the machine learning model, one has to decide whether immediate predictions, or slightly delayed predictions are desirable. These scenarios are generally referred to as:\n",
    "\n",
    "* Online (real-time): Predictions/inference happen immediately.\n",
    "* Offline (batch): Predictions/inference happen periodically. \n",
    "\n",
    "The periodic predictions can have a varying timescale too, from seconds to hours or days. \n",
    "\n",
    "These approaches can be mixed too, where our inference pipeline can happen online while the training pipeline happens in an offline fashion, which is what has been done throughout the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to deploy a machine learning model\n",
    "\n",
    "Here are a couple of options:\n",
    "\n",
    "| Tool/Resource                                  | Deployment Type               |\n",
    "|------------------------------------------------|-------------------------------|\n",
    "| Google's ML Kit                                | On-device (Android and iOS)   |\n",
    "| Apple's Core ML and coremltools Python package | On-device (all Apple devices) |\n",
    "| Amazon Web Service's (AWS) Sagemaker           | Cloud                         |\n",
    "| Google Cloud's Vertex AI                       | Cloud                         |\n",
    "| Microsoft's Azure Machine Learning             | Cloud                         |\n",
    "| Hugging Face Spaces                            | Cloud                         |\n",
    "| API with FastAPI                               | Cloud/self-hosted server      |\n",
    "| API with TorchServe                            | Cloud/self-hosted server      |\n",
    "| ONNX (Open Neural Network Exchange)            | Many/general                  |\n",
    "\n",
    "The chosen option is highly dependent on what is being built/who are you working with. \n",
    "\n",
    "One of the best small and simple ways is to turn your machine learning model into a demo app with Gradio and then deploy it on Hugging Face Spaces. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What will be covered\n",
    "\n",
    "The goal is to deploy the FoodVision Model via a demo Gradio app with the following metrics:\n",
    "1. Performance: 95% accuracy\n",
    "2. Speed: real-time inference of 30FPS+ (each prediction has a latency of lower than ~0.03s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[0]) >= 2, \"torch version should be 2.+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 15, \"torchvision version should be 0.15+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not correct. Installing correct versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "try:\n",
    "    from going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "    print(\"[INFO] Could not find going_modular scripts. Downloading them from GitHub.\")\n",
    "    !git clone https://github.com/Aaron-Serpilin/Zero-To-Mastery-Pytorch\n",
    "    !mv Zero-To-Mastery-Pytorch/Fundamentals/going_modular .\n",
    "    !mv Zero-To-Mastery-Pytorch/Fundamentals/helper_functions.py .\n",
    "    !rm -rf Zero-To-Mastery-Pytorch\n",
    "    from going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
